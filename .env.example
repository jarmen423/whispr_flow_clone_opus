# ============================================
# LocalFlow Environment Configuration
# ============================================

# Processing Mode
# Options:
#   - 'cloud': Fast cloud processing via Z.AI API (requires ZAI_API_KEY)
#   - 'networked-local': Use remote Whisper/Ollama servers on your network (default, free)
#   - 'local': Everything runs on this machine (free, requires local setup)
PROCESSING_MODE=networked-local

# ============================================
# Cloud Processing (Z.AI API)
# ============================================
# Get your API key from: https://z.ai/manage-apikey/apikey-list
# Required for cloud mode
# ZAI_API_KEY=your_api_key_here

# Z.AI API Base URL (usually no need to change)
ZAI_API_BASE_URL=https://api.z.ai/api/paas/v4

# Z.AI ASR Model for speech-to-text
# Available: glm-asr-2512
ZAI_ASR_MODEL=glm-asr-2512

# Z.AI LLM Model for text refinement
# Available: glm-4.7-flash (fast), glm-4.7 (more capable)
ZAI_LLM_MODEL=glm-4.7-flash

# ============================================
# Networked Local Mode (Remote Servers)
# ============================================
# For running Whisper and Ollama on a dedicated processing machine.
# This is the recommended default for free, private processing.

# Remote Whisper.cpp HTTP server URL
# Run on processing machine: ./server -m ggml-small.bin --host 0.0.0.0 --port 8080
# WHISPER_API_URL=http://192.168.1.100:8080

# Remote Ollama server URL (Ollama already supports remote via OLLAMA_URL)
# Start on processing machine: OLLAMA_HOST=0.0.0.0 ollama serve
OLLAMA_URL=http://localhost:11434

# ============================================
# Local Binary Mode (Same Machine)
# ============================================
# Only used when PROCESSING_MODE=local and WHISPER_API_URL is not set

# Path to Whisper.cpp binary
WHISPER_PATH=/usr/local/bin/whisper

# Path to Whisper model file
# Download from: https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-small-q5_1.bin
# Models:
#   - ggml-tiny-q5_1.bin (fastest, least accurate)
#   - ggml-small-q5_1.bin (recommended, good balance)
#   - ggml-medium-q5_1.bin (most accurate, slowest)
WHISPER_MODEL_PATH=./models/ggml-small-q5_1.bin

# Number of CPU threads for Whisper
WHISPER_THREADS=4

# ============================================
# Local Refinement (Ollama)
# ============================================
# Used for both 'networked-local' and 'local' modes

# Ollama model
# Install with: ollama pull <model>
# Models:
#   - llama3.2:1b (smallest, fastest)
#   - llama3.2:3b (good balance)
#   - llama3.2:7b (smarter, slower)
OLLAMA_MODEL=llama3.2:1b

# Temperature (lower = more deterministic)
OLLAMA_TEMPERATURE=0.1

# ============================================
# WebSocket Service
# ============================================
# Port for WebSocket service
WS_PORT=3001

# Maximum concurrent connections
WS_MAX_CONNECTIONS=100

# Heartbeat interval (milliseconds)
WS_HEARTBEAT_INTERVAL=5000

# Stale connection timeout (milliseconds)
WS_STALE_TIMEOUT=30000

# ============================================
# CORS Configuration
# ============================================
# Allowed origins for WebSocket connections
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001

# ============================================
# Application
# ============================================
# Node environment
NODE_ENV=development

# Port for Next.js application
PORT=3000

# Application URL (for callbacks if needed)
APP_URL=http://localhost:3000
