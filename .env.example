# ============================================
# LocalFlow Environment Configuration
# ============================================

# Processing Mode
# Options: 'cloud' (fast, usage-based) or 'local' (free, offline)
PROCESSING_MODE=cloud

# ============================================
# Cloud Processing (z-ai-web-dev-sdk)
# ============================================
# No configuration needed - uses cloud credentials

# ============================================
# Local Transcription (Whisper.cpp)
# ============================================
# Path to Whisper.cpp binary (for local binary mode)
WHISPER_PATH=/usr/local/bin/whisper

# Path to Whisper model file (for local binary mode)
# Download from: https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-small-q5_1.bin
# Models:
#   - ggml-tiny-q5_1.bin (fastest, least accurate)
#   - ggml-small-q5_1.bin (recommended, good balance)
#   - ggml-medium-q5_1.bin (most accurate, slowest)
WHISPER_MODEL_PATH=./models/ggml-small-q5_1.bin

# Number of CPU threads for Whisper (for local binary mode)
WHISPER_THREADS=4

# ============================================
# Networked Local Mode (Remote Whisper Server)
# ============================================
# For running whisper.cpp on a dedicated processing machine.
# If set, this takes priority over WHISPER_PATH (local binary).
# Run whisper.cpp server: ./server -m ggml-small.bin --host 0.0.0.0 --port 8080
# WHISPER_API_URL=http://192.168.1.100:8080

# ============================================
# Local Refinement (Ollama)
# ============================================
# Ollama API URL
OLLAMA_URL=http://localhost:11434

# Ollama model
# Install with: ollama pull <model>
# Models:
#   - llama3.2:1b (smallest, fastest)
#   - llama3.2:3b (good balance)
#   - llama3.2:7b (smarter, slower)
OLLAMA_MODEL=llama3.2:1b

# Temperature (lower = more deterministic)
OLLAMA_TEMPERATURE=0.1

# ============================================
# WebSocket Service
# ============================================
# Port for WebSocket service
WS_PORT=3001

# Maximum concurrent connections
WS_MAX_CONNECTIONS=100

# Heartbeat interval (milliseconds)
WS_HEARTBEAT_INTERVAL=5000

# Stale connection timeout (milliseconds)
WS_STALE_TIMEOUT=30000

# ============================================
# CORS Configuration
# ============================================
# Allowed origins for WebSocket connections
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001

# ============================================
# Application
# ============================================
# Node environment
NODE_ENV=development

# Port for Next.js application
PORT=3000

# Application URL (for callbacks if needed)
APP_URL=http://localhost:3000
